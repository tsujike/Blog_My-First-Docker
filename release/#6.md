[Docker]ノンプログラマーのためのDocker入門 Docker Compose後編（仕上げとまとめ）
Docker
 2026.02.06
どうも、ケニー（tsujikenzo）です。このシリーズでは、Docker入門をお届けしています。

前回は「Docker Compose中編」で、PythonアプリとRedisをDocker Composeで起動し、重複URLをスキップできるようにしました。

https://tgg.jugani-japan.com/tsujike/docker005

今回は、最終回です。DockerComposeの応用編をお届けします。

1. ボリュームでデータを永続化（Redis/PostgreSQL などの実データやログを保存）
2. 環境変数の使い方（.env）

## ボリュームでデータを永続化（Redis/PostgreSQL などの実データやログを保存）
データを永続化するということは、「コンテナが消えてもデータが消えない場所に保存する」ということです。
コンテナは使い捨てが前提なので、停止・削除すると中のデータも消えてしまいます。
だから「保存場所（コンテナ内/外）」を理解する必要があります。

保存場所の選択肢は大きく3つです。
- コンテナの中に保存する
- ホスト（PC）側に保存する（例: `C:\Users\Kenzo\Desktop\data`）
- ボリュームに保存する（Dockerが場所を決めて管理する）

実運用では、DBやログのように「自然に増え続けるデータ」を扱うため、
コンテナの外に保存する仕組みが必須になります。

### 「毎回CSVに出力すればいい？」がダメな理由
一時的な検証ならCSVでOKですが、
- 毎回書き出すのは面倒
- 実運用では DBやログが自動で増える
- コンテナ再作成で消えるのは困る
なので、「毎回CSVに出力する」方法は現実的ではないという話になります。

## ボリュームとは
ボリュームは、**コンテナのライフサイクルと独立してデータを残せる保存領域**です。  
コンテナを削除しても、ボリュームを消さない限りデータは残ります。

### 代表的な使いどころ
- RedisやPostgreSQLなど → `/data` や `/var/lib/postgresql/data` をマウント
- アプリのCSV → `.:/app` のようにローカルへ出力

### ボリュームの使い方
`docker-compose.yml` で `volumes` を指定すればOKです。

```yaml
services:
  redis:
    image: redis:7
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data

volumes:
  redis-data:
```

- `volumes:` セクションでボリューム名を定義します。
- Redisコンテナの `/data` をボリュームにマウントします。

### ボリュームを消したい場合

```sh
docker compose down -v
```
（`-v` を付けると関連ボリュームも削除されます）


#### Pythonアプリの取得数を500件に増やそう
それでは、`scraper.py` を修正して、ページネーションに対応し、500件以上の書籍タイトルを取得できるようにします。  
ここでは、複数ページを順次リクエストし、書籍タイトルをファイル出力するサンプルを示します。

```python
import requests
from bs4 import BeautifulSoup
import redis

def smart_scraper(max_books=500):
    r = redis.Redis(host='redis', port=6379, db=0)
    base_url = "https://books.toscrape.com/"
    current_url = base_url
    books_fetched = 0

    while current_url and books_fetched < max_books:
        response = requests.get(current_url)
        soup = BeautifulSoup(response.text, 'html.parser')
        books = soup.find_all('article', class_='product_pod')
        for book in books:
            title = book.h3.a['title']
            detail_url = book.h3.a['href']

            if r.sismember('visited_urls', detail_url):
                print(f"スキップ: 既に取得済み {title}")
                continue

            print(f"取得中...: {title}")
            r.sadd('visited_urls', detail_url)
            with open("results.csv", "a") as f:
                f.write(f"{title}\n")
            books_fetched += 1

            if books_fetched >= max_books:
                break

        next_btn = soup.find('li', class_='next')
        if next_btn and books_fetched < max_books:
            next_href = next_btn.a['href']
            if current_url == base_url:
                current_url = base_url + next_href
            else:
                import urllib.parse
                current_url = urllib.parse.urljoin(current_url, next_href)
        else:
            break

if __name__ == "__main__":
    smart_scraper()
```

- `max_books=500` で最大500冊だけ取得します（必要に応じて数値を調整してください）。
- 「次のページ」がなくなるまでページネーションし、タイトルをcsvファイルに追記します。
- Redisで重複チェックしますので、途中で中断→再実行しても、既に取得済みのURLはスキップされます。

---

### 実行する
**docker compose up --build** で起動し、コンテナ内で `scraper.py` を動かせば大量のデータ取得と永続化の仕組みが両立できています。

保存できました。






1. 環境変数の使い方（.env）
よく使う設定値（APIキーや接続先URLなど）は、.envにまとめて管理できます。
Composeは自動で.envを読み込むので、コードに直接書かずに済むのが便利です。

たとえば、今回の図書サイトのURLを `.env` に書くと以下のようになります。

```
BASE_URL=https://books.toscrape.com/
```


3. GitHub連携でイメージをリモートへ公開（GitHub Container Registry なども「できる」程度でOK）


まとめ
Docker Composeを使うと、複数コンテナをまとめて管理でき、起動・停止がとてもシンプルになります。


参考資料
Docker ドキュメント日本語化プロジェクト

このシリーズの目次
[Docker]ノンプログラマーのためのDocker入門 基礎知識編
[Docker]ノンプログラマーのためのDocker入門 Dockerコマンド編
[Docker]ノンプログラマーのためのDocker入門 Dockerfile編
[Docker]ノンプログラマーのためのDocker入門 Docker Compose前編
[Docker]ノンプログラマーのためのDocker入門 Docker Compose中編
[Docker]ノンプログラマーのためのDocker入門 Docker Compose後編
